{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thesis defense preparation | REDUX EDITION\n",
    "> 'Prepare for battle!'\n",
    "\n",
    "I should identify the weaknesses in my work. What things are missing? What things are poorly explained? In which ways are my arguments poorly based?\n",
    "\n",
    "Key weakpoints\n",
    "- My conclusion that the target instrument might be possible because I numerically optimized something without simulating it might not be convincing\n",
    "  - This is the elephant in the room\n",
    "  - SIMULATE FOIL2 PG B!!!\n",
    "    - Maybe simulate the 2 other identified interesting instruments\n",
    "- My message about sample scattering off the detector and stuff is vague\n",
    "  - Add graphs with integrated intensity over wavelengths\n",
    "  - Add actual detector profiles for radii and wavelengths like those approximated semi-analytically\n",
    "  - Try to work out corrections!\n",
    "- I did not correct for wavelength spread in my fitting, which makes their meaning unclear\n",
    "  - Work out correction\n",
    "    - At least for tau, this should be easier\n",
    "- It is unclear why my random search method should give close to optimal results\n",
    "  - Add convergence graphs\n",
    "  - Acknowledge that these should have been in an Appendix or so\n",
    "    - Frame it as an example of a key challenge in writing a good report: you have to articulate every choice you made, however obvious to you from experience \n",
    "\n",
    "- Why simulate up to found constraints, what if by simulating you could find out you can do more than that?\n",
    "  - There are soft and hard constraints in my model\n",
    "  - Identified constraints are hard constraints like device field strenghts and 1 period on detector, verified by looking at plots and corresponding B range data.\n",
    "    - Simulating further than these would mean simulating greater fields than possible given the device characteristics or measuring less than 1 full period for visibility, which is also not possible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $\\lambda$-correction for $\\tau$? Probably not a problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming a Gaussian $\\lambda$ distribution, a more correct expression for $\\sigma_t$ averaged over all wavelengths can be derived from the second moment of the distribution. $\\tau \\propto \\lambda^2$, meaning that $E[\\lambda^2]$ gives the right value. For a normal distribution $N(\\lambda_0, \\sigma^2)$ it can be derived that as $Var(\\lambda) = \\sigma = \\sqrt{E[\\lambda^2] - E[\\lambda]^2} = \\sqrt{E[\\lambda^2] - \\lambda_0^2}$, \n",
    "$$E[\\lambda^2] = \\lambda_0^2 + \\sigma^2$$\n",
    "This means that the true average $\\bar{\\tau}$ over all $\\lambda$ is related to $\\tau(\\lambda_0)$ by\n",
    "$$\\frac{\\bar{\\tau}}{\\tau(\\lambda_0)} = 1 + \\frac{\\sigma^2}{\\lambda_0^2}$$\n",
    "\n",
    "In the most extreme case, $\\Delta\\lambda/\\lambda_0 = 0.1$ which gives $\\sigma/\\lambda_0 = 0.042467$. This gives a correction factor of $1.001803$ or only about $0.2\\%$. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $Q_{max}$ and $\\lambda$ dependence? This could play a role but probably does not explain everything\n",
    "Using the wavelengths $\\lambda_0 - \\Delta\\lambda$ and $\\lambda_0 + \\Delta\\lambda$ indicates that the $Q_{max}$ values for these differs by about $22$\\% for the velocity selector, compared to only $2.02$\\% for the PG monochromator.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ratio in [0.01, 0.1]:\n",
    "    max_factor = 1 / (1 - ratio)\n",
    "    min_factor = 1 / (1 + ratio)\n",
    "    print(min_factor, max_factor, max_factor / min_factor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convergence of random search optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "# Open the file in binary mode \n",
    "with open('fit_stat_map.pkl', 'rb') as file: \n",
    "    # Call load method to deserialze \n",
    "    fit_stat_map = pickle.load(file) \n",
    "\n",
    "\n",
    "def plot_with_error_shaded(x,y,y_std,label):\n",
    "    plt.plot(x, y, '.', label=label)\n",
    "    plt.fill_between(x, y-y_std, y+y_std, alpha = 0.5)\n",
    "\n",
    "for mc_type in ['PG', 'VS']:\n",
    "    for prec_type in ['iso','foil', 'wsp','foil2',]:\n",
    "        N_range, fitness_mean, fitness_std = fit_stat_map[(prec_type,mc_type)]\n",
    "        N_start_ix = 25\n",
    "        plot_with_error_shaded(N_range[N_start_ix:], fitness_mean[N_start_ix:], fitness_std[N_start_ix:], prec_type)\n",
    "    plt.legend()\n",
    "    plt.xlabel('N')\n",
    "    plt.ylabel(r'$\\bar{g}_3$')\n",
    "    plt.title(r'$\\bar{g}_3$ for' + f' {mc_type} instruments, average over 16 runs')\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "N_range, fitness_mean, fitness_std = fit_stat_map[('foil2','PG')]\n",
    "plot_with_error_shaded(N_range[N_start_ix:], fitness_mean[N_start_ix:], fitness_std[N_start_ix:], prec_type)\n",
    "plt.legend()\n",
    "plt.xlabel('N')\n",
    "plt.ylabel(r'$\\bar{g}_3$')\n",
    "plt.title(r'$\\bar{g}_3$ for FOIL2 PG, average over 16 runs')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.-1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
